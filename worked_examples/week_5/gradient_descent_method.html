
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Worked Examples: Gradient Descent Method &#8212; CH40208: Topics in Computational Chemistry</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/ch40208.css?v=7039b35b" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"TeX": {"Macros": {"bvec": ["\\mathbf{#1}", 1], "cvec": ["\\begin{bmatrix}#1\\\\#2\\end{bmatrix}", 2], "norm": ["\\left\\lVert #1\\right\\rVert", 1], "tmatrix": ["\\begin{bmatrix}#1&#2\\\\#3&#4\\end{bmatrix}", 4]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'worked_examples/week_5/gradient_descent_method';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Worked Examples: Newton-Raphson Method" href="newton_raphson_method.html" />
    <link rel="prev" title="Worked Examples: Grid Search Method" href="grid_search_method.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../introduction/about_this_book.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/CH40208_image.png" class="logo__image only-light" alt="CH40208: Topics in Computational Chemistry - Home"/>
    <script>document.write(`<img src="../../_static/CH40208_image.png" class="logo__image only-dark" alt="CH40208: Topics in Computational Chemistry - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../introduction/about_this_book.html">
                    About this Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../introduction/course_outline.html">CH40208 course contents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../course_contents/week_1.html">Week 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course_contents/week_2.html">Week 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course_contents/week_3.html">Week 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course_contents/week_4.html">Week 4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course_contents/week_5.html">Week 5</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course_contents/week_6.html">Week 6</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course_contents/week_7.html">Week 7</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course_contents/week_8.html">Week 8</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course_contents/week_9.html">Week 9</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notebooks_introduction/what_is_a_jupyter_notebook.html">What is a Jupyter Notebook?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks_introduction/using_noteable.html">Getting started with Noteable</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks_introduction/the_notebook_interface.html">The Jupyter Notebook interface</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/python_first_chemistry_later.html">Python First, Computational Chemistry Later</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/why_we_encourage_typing_out_the_code_examples.html">Why We Encourage Typing Out the Code Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/using_Python_as_a_calculator.html">Using Python as a calculator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/getting_started_with_strings.html">Getting started with strings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/print_statements.html">Print statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/types.html">Types</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../python_basics/variables.html">Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../python_basics/variable_naming_conventions.html">Naming Conventions for Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/arithmetic.html">Arithmetic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/lists.html">Lists</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/loops.html">Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/tuples.html">Tuples</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../python_basics/dictionaries.html">Dictionaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../python_basics/iterating_over_dictionaries.html">Iterating over Dictionaries</a></li>

</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/functions.html">Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/comparisons_and_flow_control.html">Comparisons and Flow Control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python_basics/advanced_loop_control.html">Advanced Loop Control</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Working with Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../working_with_data/numpy/intro.html">NumPy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../working_with_data/numpy/basics.html">NumPy Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../working_with_data/numpy/operations.html">Array Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../working_with_data/numpy/exercises.html">NumPy Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../working_with_data/matplotlib/intro.html">Matplotlib</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../working_with_data/matplotlib/basics.html">Matplotlib Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../working_with_data/matplotlib/customisation.html">Customising Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../working_with_data/matplotlib/plotting_data.html">Plotting Data</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/latex.html">LaTeX Formatting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../working_with_data/file_io.html">File input and output</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../working_with_data/synoptic_exercises.html">Synoptic exercises</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../working_with_data/H_emission_exercise.html">Modelling the Hydrogen Emission Spectrum</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../model_fitting/model_fitting.html">Model Fitting</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../model_fitting/statistical_models.html">Statistical Models: Turning Data into Understanding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../model_fitting/empirical_and_theoretical_models.html">Empirical and Theoretical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../model_fitting/model_complexity.html">Model Complexity and the Goal of Fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../model_fitting/finding_the_best_fit.html">Finding the Best Fit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../model_fitting/understanding_residuals.html">Understanding Residuals and Quality of Fit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../model_fitting/fitting_with_minimize.html">Linear Model Fitting with <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../model_fitting/exercises.html">Exercises</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Good Practice</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../good_practice/readable.html">Writing readable code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../good_practice/modularisation.html">Modularisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../good_practice/markdown.html">Markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computational Chemistry Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../geometry_optimisation/geometry_optimisation.html">Geometry Optimisation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../geometry_optimisation/analytical_solution_for_a_harmonic_potential.html">Analytical Solution for a Harmonic Potential</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../geometry_optimisation/grid_search_method.html">Grid Search Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../geometry_optimisation/gradient_descent_method.html">Gradient Descent Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../geometry_optimisation/newton_raphson_method.html">The Newton-Raphson Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../geometry_optimisation/Lennard_Jones_optimisation.html">Synoptic Exercise: Geometry Optimisation of a Lennard-Jones Potential</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../geometry_optimisation/scipy_optimize_minimize.html">Minimisation with <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code></a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../molecular_dynamics/molecular_dynamics.html">Molecular Dynamics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../molecular_dynamics/understanding_molecular_dynamics.html">Understanding Molecular Dynamics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../molecular_dynamics/the_mechanics_of_molecular_dynamics.html">The Mechanics of Molecular Dynamics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../molecular_dynamics/numerical_integration_methods.html">Numerical Integration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../molecular_dynamics/choosing_the_right_timestep.html">Choosing the Right Timestep</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../molecular_dynamics/moving_to_real_systems.html">Moving to Real Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../molecular_dynamics/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../monte_carlo/monte_carlo.html">Monte Carlo Simulation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../monte_carlo/estimating_pi.html">Estimating <span class="math notranslate nohighlight">\(\pi\)</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../monte_carlo/metropolis_monte_carlo.html">Sampling Molecular Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../monte_carlo/the_metropolis_algorithm.html">The Metropolis Algorithm</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../monte_carlo/ising_model_simulation.html">Monte Carlo Simulation of the 1D Ising Model</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../vectors_and_matrices/vectors_and_matrices.html">Working with vectors and matrices</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../vectors_and_matrices/vectors.html">Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../vectors_and_matrices/vectors_in_python.html">Working with vectors in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../vectors_and_matrices/matrices.html">Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../vectors_and_matrices/matrices_in_python.html">Working with matrices in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../vectors_and_matrices/eigenvalues_and_eigenvectors.html">Eigenvalues and eigenvectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../vectors_and_matrices/moments_of_inertia.html">Principal rotation axes and principal moments of inertia</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Example Coursework Notebooks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../example_coursework_notebooks/ir_spectra.html">Determination of the HCl bond length by IR spectroscopy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Planning Your Code</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../code_schematics/code_schematics.html">Planning Your Code and Writing A Code Schematic</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../reference/adding_files_to_noteable.html">Adding Files to Noteable</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercise Worked Examples</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week_1_index.html">Week 1</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week_1_calculator.html">Using Python as a Calculator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week_1_strings.html">Getting Started with Strings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week_1_arithmetic.html">Arithmetic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week_1_lists.html">Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week_1_loops.html">Loops</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week_1_dictionaries.html">Dictionaries</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week_2_index.html">Week 2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week_2_functions.html">Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week_2_comparisons_and_flow_control.html">Comparisons and Flow Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week_2_advanced_loop_control.html">Advanced Loop Control</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week_3_index.html">Week 3</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week_3_numpy.html">NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week_3_synoptic_exercises.html">Synoptic Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="week_5_index.html">Week 5</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="grid_search_method.html">Grid Search</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="newton_raphson_method.html">Newton-Raphson</a></li>
<li class="toctree-l2"><a class="reference internal" href="Lennard_Jones_optimisation.html">Lennard-Jones optimisation</a></li>
<li class="toctree-l2"><a class="reference internal" href="scipy_optimize_minimize.html">scipy.optimize.minimize</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week_6/week_6_index.html">Week 6</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week_6/week_6_worked_example.html">Worked Example Solutions</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/worked_examples/week_5/gradient_descent_method.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Worked Examples: Gradient Descent Method</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-fixed-step-size-gradient-descent">Exercise: Fixed Step Size Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-define-functions">Setup: Define functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-fixed-step-size-r-0-01-a">Part 1: Fixed step size Δ<em>r</em> = 0.01 Å</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-fixed-step-size-r-0-1-a">Part 2: Fixed step size Δr = 0.1 Å</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-oscillation">Visualizing the oscillation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-adaptive-step-size-gradient-descent">Exercise: Adaptive Step Size Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-learning-rate-0-01">Part 1: Learning rate α = 0.01</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-learning-rate-0-001">Part 2: Learning rate α = 0.001</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-learning-rate-0-1">Part 3: Learning rate α = 0.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-choosing-the-learning-rate">Summary: Choosing the learning rate</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="worked-examples-gradient-descent-method">
<h1>Worked Examples: Gradient Descent Method<a class="headerlink" href="#worked-examples-gradient-descent-method" title="Link to this heading">#</a></h1>
<p>These worked solutions correspond to the exercises on the <a class="reference internal" href="#../geometry_optimisation/gradient_descent_method.ipynb"><span class="xref myst">Gradient Descent Method</span></a> page.</p>
<p><strong>How to use this notebook:</strong></p>
<ul class="simple">
<li><p>Try each exercise yourself first before looking at the solution</p></li>
<li><p>The code cells show both the code and its output</p></li>
<li><p>Download this notebook if you want to run and experiment with the code yourself</p></li>
<li><p>Your solution might look different - that’s fine as long as it gives the correct answer!</p></li>
</ul>
<section id="exercise-fixed-step-size-gradient-descent">
<h2>Exercise: Fixed Step Size Gradient Descent<a class="headerlink" href="#exercise-fixed-step-size-gradient-descent" title="Link to this heading">#</a></h2>
<p><strong>Problem:</strong> Implement gradient descent with a fixed step size to find the minimum of the harmonic potential energy surface. We need to first write a function to calculate the first derivative of the harmonic potential:</p>
<div class="math notranslate nohighlight">
\[U'(r) = k(r - r_0)\]</div>
<p>Then implement the gradient descent algorithm using:</p>
<div class="math notranslate nohighlight">
\[r_{i+1} = r_i - \Delta r \times \mathrm{sign}(U'(r_i))\]</div>
<p>We’ll start from <span class="math notranslate nohighlight">\(r = 1.0\)</span> Å and explore how different step sizes affect convergence.</p>
<section id="setup-define-functions">
<h3>Setup: Define functions<a class="headerlink" href="#setup-define-functions" title="Link to this heading">#</a></h3>
<p>First, we’ll define our harmonic potential and its derivative:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format=&#39;retina&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">harmonic_potential</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the harmonic potential energy.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        r (float): Bond length (Å). Can be a float or np.ndarray.</span>
<span class="sd">        k (float): Force constant (eV Å^-2).</span>
<span class="sd">        r_0 (float): Equilibrium bond length (Å).</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        Potential energy (eV). Type matches input r.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">r_0</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">harmonic_gradient</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the first derivative of the harmonic potential.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        rr (float): Bond length (Å). Can be a float or np.ndarray.</span>
<span class="sd">        k (float): Force constant (eV Å^-2).</span>
<span class="sd">        r_0 (float): Equilibrium bond length (Å).</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        First derivative dU/dr (eV Å^-1). Type matches input r.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">r_0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Parameters for H₂</span>
<span class="n">k</span> <span class="o">=</span> <span class="mf">36.0</span>  <span class="c1"># eV Å^-2</span>
<span class="n">r_0</span> <span class="o">=</span> <span class="mf">0.74</span>  <span class="c1"># Å</span>

<span class="c1"># Test the gradient function</span>
<span class="n">r_test</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Å</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="n">harmonic_gradient</span><span class="p">(</span><span class="n">r_test</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient at r = </span><span class="si">{</span><span class="n">r_test</span><span class="si">}</span><span class="s2"> Å: U&#39;(r) = </span><span class="si">{</span><span class="n">gradient</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> eV Å^-1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Gradient at r = 1.0 Å: U&#39;(r) = 9.360 eV Å^-1
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-1-fixed-step-size-r-0-01-a">
<h3>Part 1: Fixed step size Δ<em>r</em> = 0.01 Å<a class="headerlink" href="#part-1-fixed-step-size-r-0-01-a" title="Link to this heading">#</a></h3>
<p>Let’s implement the gradient descent algorithm with a small step size and see how it performs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradient descent parameters</span>
<span class="n">r_start</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Å</span>
<span class="n">delta_r</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># Å (step size)</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">convergence_threshold</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># eV Å⁻¹</span>

<span class="c1"># Initialize</span>
<span class="n">r_current</span> <span class="o">=</span> <span class="n">r_start</span>
<span class="n">r_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">r_current</span><span class="p">]</span>
<span class="n">gradient_history</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting gradient descent with Δr = </span><span class="si">{</span><span class="n">delta_r</span><span class="si">}</span><span class="s2"> Å</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration | Position (Å) | Gradient (eV Å⁻¹)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">converged</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
    <span class="c1"># Calculate gradient at current position</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">harmonic_gradient</span><span class="p">(</span><span class="n">r_current</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span><span class="p">)</span>
    <span class="n">gradient_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">iteration</span><span class="si">:</span><span class="s2">9d</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">r_current</span><span class="si">:</span><span class="s2">12.6f</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">gradient</span><span class="si">:</span><span class="s2">18.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Check for convergence</span>
    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">convergence_threshold</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged after </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2"> iterations!&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final position: r = </span><span class="si">{</span><span class="n">r_current</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> Å&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Analytical solution: r₀ = </span><span class="si">{</span><span class="n">r_0</span><span class="si">}</span><span class="s2"> Å&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">r_current</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">r_0</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> Å&quot;</span><span class="p">)</span>
        <span class="n">converged</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">break</span>
    
    <span class="c1"># Update position (gradient descent step)</span>
    <span class="n">r_current</span> <span class="o">=</span> <span class="n">r_current</span> <span class="o">-</span> <span class="n">delta_r</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
    <span class="n">r_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r_current</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">converged</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Did not converge within </span><span class="si">{</span><span class="n">max_iterations</span><span class="si">}</span><span class="s2"> iterations.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final position: r = </span><span class="si">{</span><span class="n">r_current</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> Å&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final gradient: </span><span class="si">{</span><span class="n">gradient</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> eV Å⁻¹&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Starting gradient descent with Δr = 0.01 Å

Iteration | Position (Å) | Gradient (eV Å⁻¹)
--------------------------------------------------
        0 |     1.000000 |           9.360000
        1 |     0.990000 |           9.000000
        2 |     0.980000 |           8.640000
        3 |     0.970000 |           8.280000
        4 |     0.960000 |           7.920000
        5 |     0.950000 |           7.560000
        6 |     0.940000 |           7.200000
        7 |     0.930000 |           6.840000
        8 |     0.920000 |           6.480000
        9 |     0.910000 |           6.120000
       10 |     0.900000 |           5.760000
       11 |     0.890000 |           5.400000
       12 |     0.880000 |           5.040000
       13 |     0.870000 |           4.680000
       14 |     0.860000 |           4.320000
       15 |     0.850000 |           3.960000
       16 |     0.840000 |           3.600000
       17 |     0.830000 |           3.240000
       18 |     0.820000 |           2.880000
       19 |     0.810000 |           2.520000
       20 |     0.800000 |           2.160000
       21 |     0.790000 |           1.800000
       22 |     0.780000 |           1.440000
       23 |     0.770000 |           1.080000
       24 |     0.760000 |           0.720000
       25 |     0.750000 |           0.360000
       26 |     0.740000 |          -0.000000

Converged after 26 iterations!
Final position: r = 0.740000 Å
Analytical solution: r₀ = 0.74 Å
Error: 0.000000 Å
</pre></div>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<p>With <span class="math notranslate nohighlight">\(\Delta r = 0.01\)</span> Å, the algorithm <strong>converges successfully</strong> in 26 iterations. Key observations:</p>
<ol class="arabic simple">
<li><p><strong>Steady convergence</strong>: The position moves consistently towards the minimum at <span class="math notranslate nohighlight">\(r_0 = 0.74\)</span> Å.</p></li>
<li><p><strong>Fixed step size</strong>: Each iteration moves exactly 0.01 Å in the direction indicated by the gradient sign:</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(r &gt; r_0\)</span>: gradient is positive, so we step left (decrease <span class="math notranslate nohighlight">\(r\)</span>)</p></li>
<li><p>When <span class="math notranslate nohighlight">\(r &lt; r_0\)</span>: gradient is negative, so we step right (increase <span class="math notranslate nohighlight">\(r\)</span>)</p></li>
</ul>
</li>
<li><p><strong>Final accuracy</strong>: The algorithm stops when <span class="math notranslate nohighlight">\(|U'(r)| &lt; 0.001\)</span> eV Å⁻¹, which occurs at approximately <span class="math notranslate nohighlight">\(r = 0.74\)</span> Å—exactly the analytical minimum!</p></li>
<li><p><strong>Why 26 iterations?</strong>: Starting at <span class="math notranslate nohighlight">\(r = 1.0\)</span> Å, we need to travel <span class="math notranslate nohighlight">\(0.26\)</span> Å to reach <span class="math notranslate nohighlight">\(r_0 = 0.74\)</span> Å. With steps of 0.01 Å, this requires about <span class="math notranslate nohighlight">\(0.26 / 0.01 = 26\)</span> steps.</p></li>
</ol>
<p>The method is straightforward but slow—each small step requires a function evaluation.</p>
</section>
<section id="part-2-fixed-step-size-r-0-1-a">
<h3>Part 2: Fixed step size Δr = 0.1 Å<a class="headerlink" href="#part-2-fixed-step-size-r-0-1-a" title="Link to this heading">#</a></h3>
<p>Now let’s try a larger step size to see if we can converge faster:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Try larger step size</span>
<span class="n">r_start</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># Å</span>
<span class="n">delta_r</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Å (larger step size)</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">convergence_threshold</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># eV Å⁻¹</span>

<span class="c1"># Initialize</span>
<span class="n">r_current</span> <span class="o">=</span> <span class="n">r_start</span>
<span class="n">r_history_large</span> <span class="o">=</span> <span class="p">[</span><span class="n">r_current</span><span class="p">]</span>
<span class="n">gradient_history_large</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting gradient descent with Δr = </span><span class="si">{</span><span class="n">delta_r</span><span class="si">}</span><span class="s2"> Å</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration | Position (Å) | Gradient (eV Å⁻¹)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">converged</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
    <span class="c1"># Calculate gradient at current position</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">harmonic_gradient</span><span class="p">(</span><span class="n">r_current</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span><span class="p">)</span>
    <span class="n">gradient_history_large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">iteration</span><span class="si">:</span><span class="s2">9d</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">r_current</span><span class="si">:</span><span class="s2">12.6f</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">gradient</span><span class="si">:</span><span class="s2">18.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Check for convergence</span>
    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">convergence_threshold</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged after </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2"> iterations!&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final position: r = </span><span class="si">{</span><span class="n">r_current</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> Å&quot;</span><span class="p">)</span>
        <span class="n">converged</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">break</span>
    
    <span class="c1"># Update position</span>
    <span class="n">r_current</span> <span class="o">=</span> <span class="n">r_current</span> <span class="o">-</span> <span class="n">delta_r</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
    <span class="n">r_history_large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r_current</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">converged</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Did not converge within </span><span class="si">{</span><span class="n">max_iterations</span><span class="si">}</span><span class="s2"> iterations.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Starting gradient descent with Δr = 0.1 Å

Iteration | Position (Å) | Gradient (eV Å⁻¹)
--------------------------------------------------
        0 |     1.000000 |           9.360000
        1 |     0.900000 |           5.760000
        2 |     0.800000 |           2.160000
        3 |     0.700000 |          -1.440000
        4 |     0.800000 |           2.160000
        5 |     0.700000 |          -1.440000
        6 |     0.800000 |           2.160000
        7 |     0.700000 |          -1.440000
        8 |     0.800000 |           2.160000
        9 |     0.700000 |          -1.440000
       10 |     0.800000 |           2.160000
       11 |     0.700000 |          -1.440000
       12 |     0.800000 |           2.160000
       13 |     0.700000 |          -1.440000
       14 |     0.800000 |           2.160000
       15 |     0.700000 |          -1.440000
       16 |     0.800000 |           2.160000
       17 |     0.700000 |          -1.440000
       18 |     0.800000 |           2.160000
       19 |     0.700000 |          -1.440000
       20 |     0.800000 |           2.160000
       21 |     0.700000 |          -1.440000
       22 |     0.800000 |           2.160000
       23 |     0.700000 |          -1.440000
       24 |     0.800000 |           2.160000
       25 |     0.700000 |          -1.440000
       26 |     0.800000 |           2.160000
       27 |     0.700000 |          -1.440000
       28 |     0.800000 |           2.160000
       29 |     0.700000 |          -1.440000
       30 |     0.800000 |           2.160000
       31 |     0.700000 |          -1.440000
       32 |     0.800000 |           2.160000
       33 |     0.700000 |          -1.440000
       34 |     0.800000 |           2.160000
       35 |     0.700000 |          -1.440000
       36 |     0.800000 |           2.160000
       37 |     0.700000 |          -1.440000
       38 |     0.800000 |           2.160000
       39 |     0.700000 |          -1.440000
       40 |     0.800000 |           2.160000
       41 |     0.700000 |          -1.440000
       42 |     0.800000 |           2.160000
       43 |     0.700000 |          -1.440000
       44 |     0.800000 |           2.160000
       45 |     0.700000 |          -1.440000
       46 |     0.800000 |           2.160000
       47 |     0.700000 |          -1.440000
       48 |     0.800000 |           2.160000
       49 |     0.700000 |          -1.440000

Did not converge within 50 iterations.
</pre></div>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<p>With <span class="math notranslate nohighlight">\(\Delta r = 0.1\)</span> Å, the algorithm <strong>fails to converge</strong> and instead <strong>oscillates</strong> around the minimum. Here’s why:</p>
<ol class="arabic simple">
<li><p><strong>Overshooting</strong>: Starting at <span class="math notranslate nohighlight">\(r = 1.0\)</span> Å, the minimum is at <span class="math notranslate nohighlight">\(r_0 = 0.74\)</span> Å, a distance of only 0.26 Å. A step size of 0.1 Å is too large relative to this distance.</p></li>
<li><p><strong>The oscillation pattern</strong>:</p>
<ul class="simple">
<li><p><strong>Iteration 0</strong>: <span class="math notranslate nohighlight">\(r = 1.0\)</span> Å → gradient positive → step left</p></li>
<li><p><strong>Iteration 1</strong>: <span class="math notranslate nohighlight">\(r = 0.9\)</span> Å (overshot to the left side of minimum)</p></li>
<li><p><strong>Iteration 2</strong>: <span class="math notranslate nohighlight">\(r = 0.8\)</span> Å (gradient now negative, step right)</p></li>
<li><p><strong>Iteration 3</strong>: <span class="math notranslate nohighlight">\(r = 0.9\)</span> Å (back to the right side)</p></li>
<li><p>The algorithm bounces back and forth between <span class="math notranslate nohighlight">\(r = 0.8\)</span> Å and <span class="math notranslate nohighlight">\(r = 0.9\)</span> Å</p></li>
</ul>
</li>
<li><p><strong>Why no convergence?</strong>: The fixed 0.1 Å steps are too large to “land” close enough to the minimum where <span class="math notranslate nohighlight">\(|U'(r)| &lt; 0.001\)</span> eV Å⁻¹. The algorithm perpetually overshoots.</p></li>
</ol>
<p>This demonstrates a fundamental limitation of fixed step size gradient descent: the step size must be carefully chosen, and what works for one starting position may not work for another.</p>
</section>
<section id="visualizing-the-oscillation">
<h3>Visualizing the oscillation<a class="headerlink" href="#visualizing-the-oscillation" title="Link to this heading">#</a></h3>
<p>Let’s plot both convergence paths to see the difference clearly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, recreate the successful run with small step size for plotting</span>
<span class="n">r_current</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">r_history_small</span> <span class="o">=</span> <span class="p">[</span><span class="n">r_current</span><span class="p">]</span>
<span class="n">delta_r_small</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">26</span><span class="p">):</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">harmonic_gradient</span><span class="p">(</span><span class="n">r_current</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.001</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">r_current</span> <span class="o">=</span> <span class="n">r_current</span> <span class="o">-</span> <span class="n">delta_r_small</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
    <span class="n">r_history_small</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r_current</span><span class="p">)</span>

<span class="c1"># Plot the small step size (converges)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r_history_small</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Position vs iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">r_0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Minimum at r₀ = </span><span class="si">{</span><span class="n">r_0</span><span class="si">}</span><span class="s1"> Å&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Position, $r$ (Å)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Gradient Descent with Δr = 0.01 Å (Converges)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot the large step size (oscillates)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r_history_large</span><span class="p">,</span> <span class="s1">&#39;go-&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Position vs iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">r_0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Minimum at r₀ = </span><span class="si">{</span><span class="n">r_0</span><span class="si">}</span><span class="s1"> Å&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Position, $r$ (Å)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Gradient Descent with Δr = 0.1 Å (Oscillates)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../_images/463b9d2b2adeac1eb8c1f94685652488cf6e566cd6414f919249f3bdf2e03d4d.png"><img alt="../../_images/463b9d2b2adeac1eb8c1f94685652488cf6e566cd6414f919249f3bdf2e03d4d.png" src="../../_images/463b9d2b2adeac1eb8c1f94685652488cf6e566cd6414f919249f3bdf2e03d4d.png" style="width: 579px; height: 457px;" />
</a>
<a class="reference internal image-reference" href="../../_images/84a53c5dd2fe0342f22c9a610af08030575d8ec0c01bf8e063b52d11c58902fd.png"><img alt="../../_images/84a53c5dd2fe0342f22c9a610af08030575d8ec0c01bf8e063b52d11c58902fd.png" src="../../_images/84a53c5dd2fe0342f22c9a610af08030575d8ec0c01bf8e063b52d11c58902fd.png" style="width: 579px; height: 455px;" />
</a>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="exercise-adaptive-step-size-gradient-descent">
<h2>Exercise: Adaptive Step Size Gradient Descent<a class="headerlink" href="#exercise-adaptive-step-size-gradient-descent" title="Link to this heading">#</a></h2>
<p>The fixed step size method showed us that choosing an appropriate step size is crucial but difficult. An adaptive approach rescales the step size based on the local gradient magnitude:</p>
<div class="math notranslate nohighlight">
\[r_{i+1} = r_i - \alpha U'(r_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the <strong>learning rate</strong> parameter. This approach naturally takes smaller steps near the minimum (where the gradient is small) and larger steps far from the minimum (where the gradient is large).</p>
<section id="part-1-learning-rate-0-01">
<h3>Part 1: Learning rate α = 0.01<a class="headerlink" href="#part-1-learning-rate-0-01" title="Link to this heading">#</a></h3>
<p>Let’s implement adaptive step size gradient descent:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gradient_descent_adaptive</span><span class="p">(</span><span class="n">r_start</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform adaptive step size gradient descent.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        r_start (float): Starting position (Å).</span>
<span class="sd">        alpha (float): Learning rate.</span>
<span class="sd">        k (float): Force constant (eV Å⁻²).</span>
<span class="sd">        r_0 (float): Equilibrium bond length (Å).</span>
<span class="sd">        max_iterations (int): Maximum number of iterations.</span>
<span class="sd">        threshold (float): Convergence threshold for |gradient|.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        tuple: (r_history, gradient_history, converged, num_iterations).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">r_current</span> <span class="o">=</span> <span class="n">r_start</span>
    <span class="n">r_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">r_current</span><span class="p">]</span>
    <span class="n">gradient_history</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Adaptive gradient descent with α = </span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration | Position (Å) | Gradient (eV Å⁻¹) |     Δr (Å)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">65</span><span class="p">)</span>
    
    <span class="n">converged</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        <span class="c1"># Calculate gradient</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">harmonic_gradient</span><span class="p">(</span><span class="n">r_current</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span><span class="p">)</span>
        <span class="n">gradient_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
        
        <span class="c1"># Calculate step size (adaptive)</span>
        <span class="n">step</span> <span class="o">=</span> <span class="o">-</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">gradient</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">iteration</span><span class="si">:</span><span class="s2">9d</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">r_current</span><span class="si">:</span><span class="s2">12.6f</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">gradient</span><span class="si">:</span><span class="s2">17.6f</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">step</span><span class="si">:</span><span class="s2">9.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Check convergence</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Converged after </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2"> iterations!&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final position: r = </span><span class="si">{</span><span class="n">r_current</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> Å&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Analytical solution: r_0 = </span><span class="si">{</span><span class="n">r_0</span><span class="si">}</span><span class="s2"> Å&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="nb">abs</span><span class="p">(</span><span class="n">r_current</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">r_0</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> Å&quot;</span><span class="p">)</span>
            <span class="n">converged</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">break</span>
        
        <span class="c1"># Update position</span>
        <span class="n">r_current</span> <span class="o">=</span> <span class="n">r_current</span> <span class="o">+</span> <span class="n">step</span>
        <span class="n">r_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r_current</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">converged</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Did not converge within </span><span class="si">{</span><span class="n">max_iterations</span><span class="si">}</span><span class="s2"> iterations.&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">r_history</span><span class="p">,</span> <span class="n">gradient_history</span><span class="p">,</span> <span class="n">converged</span><span class="p">,</span> <span class="n">iteration</span>

<span class="c1"># Run with α = 0.01</span>
<span class="n">r_history_01</span><span class="p">,</span> <span class="n">grad_history_01</span><span class="p">,</span> <span class="n">converged_01</span><span class="p">,</span> <span class="n">iters_01</span> <span class="o">=</span> <span class="n">gradient_descent_adaptive</span><span class="p">(</span>
    <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adaptive gradient descent with α = 0.01

Iteration | Position (Å) | Gradient (eV Å⁻¹) |     Δr (Å)
-----------------------------------------------------------------
        0 |     1.000000 |          9.360000 | -0.093600
        1 |     0.906400 |          5.990400 | -0.059904
        2 |     0.846496 |          3.833856 | -0.038339
        3 |     0.808157 |          2.453668 | -0.024537
        4 |     0.783621 |          1.570347 | -0.015703
        5 |     0.767917 |          1.005022 | -0.010050
        6 |     0.757867 |          0.643214 | -0.006432
        7 |     0.751435 |          0.411657 | -0.004117
        8 |     0.747318 |          0.263461 | -0.002635
        9 |     0.744684 |          0.168615 | -0.001686
       10 |     0.742998 |          0.107913 | -0.001079
       11 |     0.741918 |          0.069065 | -0.000691
       12 |     0.741228 |          0.044201 | -0.000442
       13 |     0.740786 |          0.028289 | -0.000283
       14 |     0.740503 |          0.018105 | -0.000181
       15 |     0.740322 |          0.011587 | -0.000116
       16 |     0.740206 |          0.007416 | -0.000074
       17 |     0.740132 |          0.004746 | -0.000047
       18 |     0.740084 |          0.003037 | -0.000030
       19 |     0.740054 |          0.001944 | -0.000019
       20 |     0.740035 |          0.001244 | -0.000012
       21 |     0.740022 |          0.000796 | -0.000008

Converged after 21 iterations!
Final position: r = 0.740022 Å
Analytical solution: r_0 = 0.74 Å
Error: 0.000022 Å
</pre></div>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<p>With <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>, the algorithm converges successfully. Let’s examine how the adaptive step sizes work:</p>
<p>Key observations:</p>
<ol class="arabic simple">
<li><p><strong>Adaptive step sizes</strong>: Notice how the “Δr (Å)” column changes:</p>
<ul class="simple">
<li><p><strong>Early iterations</strong>: When far from the minimum, the gradient is large (e.g., 9.36 eV Å<sup>−1</sup> initially), so steps are large (−0.0936 Å)</p></li>
<li><p><strong>Later iterations</strong>: As we approach the minimum, the gradient decreases, and steps become progressively smaller</p></li>
<li><p><strong>Near convergence</strong>: Steps become very small as the gradient approaches zero</p></li>
</ul>
</li>
<li><p><strong>Natural damping</strong>: The step size automatically decreases as we approach the minimum because the gradient magnitude decreases. This prevents oscillation.</p></li>
<li><p><strong>Efficiency</strong>: The adaptive method allows large initial steps when far from the minimum, then automatically refines with smaller steps near convergence, without needing to manually tune the step size.</p></li>
<li><p><strong>Why it works</strong>: For the harmonic potential, <span class="math notranslate nohighlight">\(U'(r) = k(r - r_0)\)</span>, so the gradient is directly proportional to the distance from the minimum. The adaptive step size therefore automatically scales with this distance.</p></li>
</ol>
</section>
<section id="part-2-learning-rate-0-001">
<h3>Part 2: Learning rate α = 0.001<a class="headerlink" href="#part-2-learning-rate-0-001" title="Link to this heading">#</a></h3>
<p>What happens with a smaller learning rate?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run with α = 0.001 (smaller)</span>
<span class="n">r_history_001</span><span class="p">,</span> <span class="n">grad_history_001</span><span class="p">,</span> <span class="n">converged_001</span><span class="p">,</span> <span class="n">iters_001</span> <span class="o">=</span> <span class="n">gradient_descent_adaptive</span><span class="p">(</span>
    <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adaptive gradient descent with α = 0.001

Iteration | Position (Å) | Gradient (eV Å⁻¹) |     Δr (Å)
-----------------------------------------------------------------
        0 |     1.000000 |          9.360000 | -0.009360
        1 |     0.990640 |          9.023040 | -0.009023
        2 |     0.981617 |          8.698211 | -0.008698
        3 |     0.972919 |          8.385075 | -0.008385
        4 |     0.964534 |          8.083212 | -0.008083
        5 |     0.956450 |          7.792217 | -0.007792
        6 |     0.948658 |          7.511697 | -0.007512
        7 |     0.941147 |          7.241276 | -0.007241
        8 |     0.933905 |          6.980590 | -0.006981
        9 |     0.926925 |          6.729289 | -0.006729
       10 |     0.920195 |          6.487034 | -0.006487
       11 |     0.913708 |          6.253501 | -0.006254
       12 |     0.907455 |          6.028375 | -0.006028
       13 |     0.901426 |          5.811353 | -0.005811
       14 |     0.895615 |          5.602145 | -0.005602
       15 |     0.890013 |          5.400468 | -0.005400
       16 |     0.884613 |          5.206051 | -0.005206
       17 |     0.879406 |          5.018633 | -0.005019
       18 |     0.874388 |          4.837962 | -0.004838
       19 |     0.869550 |          4.663795 | -0.004664
       20 |     0.864886 |          4.495899 | -0.004496
       21 |     0.860390 |          4.334046 | -0.004334
       22 |     0.856056 |          4.178021 | -0.004178
       23 |     0.851878 |          4.027612 | -0.004028
       24 |     0.847850 |          3.882618 | -0.003883
       25 |     0.843968 |          3.742844 | -0.003743
       26 |     0.840225 |          3.608101 | -0.003608
       27 |     0.836617 |          3.478210 | -0.003478
       28 |     0.833139 |          3.352994 | -0.003353
       29 |     0.829786 |          3.232286 | -0.003232
       30 |     0.826553 |          3.115924 | -0.003116
       31 |     0.823438 |          3.003751 | -0.003004
       32 |     0.820434 |          2.895616 | -0.002896
       33 |     0.817538 |          2.791374 | -0.002791
       34 |     0.814747 |          2.690884 | -0.002691
       35 |     0.812056 |          2.594012 | -0.002594
       36 |     0.809462 |          2.500628 | -0.002501
       37 |     0.806961 |          2.410605 | -0.002411
       38 |     0.804551 |          2.323823 | -0.002324
       39 |     0.802227 |          2.240166 | -0.002240
       40 |     0.799987 |          2.159520 | -0.002160
       41 |     0.797827 |          2.081777 | -0.002082
       42 |     0.795745 |          2.006833 | -0.002007
       43 |     0.793739 |          1.934587 | -0.001935
       44 |     0.791804 |          1.864942 | -0.001865
       45 |     0.789939 |          1.797804 | -0.001798
       46 |     0.788141 |          1.733083 | -0.001733
       47 |     0.786408 |          1.670692 | -0.001671
       48 |     0.784737 |          1.610547 | -0.001611
       49 |     0.783127 |          1.552568 | -0.001553

Did not converge within 50 iterations.
</pre></div>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<p>With <span class="math notranslate nohighlight">\(\alpha = 0.001\)</span>, the algorithm <strong>does not converge</strong> within 50 iterations. After 49 iterations, the gradient is still 1.55 eV Å⁻¹, well above the convergence threshold of 0.001 eV Å⁻¹.</p>
<p>The problem:</p>
<ol class="arabic simple">
<li><p><strong>Too conservative</strong>: With such a small learning rate, each step is very cautious:</p>
<ul class="simple">
<li><p><strong>Iteration 0</strong>: Step is only −0.00936 Å (compared to −0.0936 Å with <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>)</p></li>
<li><p>After 49 iterations, we’ve only reached <span class="math notranslate nohighlight">\(r = 0.783\)</span> Å, still 0.043 Å away from the minimum at <span class="math notranslate nohighlight">\(r_0 = 0.74\)</span> Å</p></li>
</ul>
</li>
<li><p><strong>Progress is too slow</strong>: The gradient decreases very gradually from 9.36 eV Å⁻¹ to 1.55 eV Å⁻¹, but we’re making progress at such a slow rate that we can’t reach convergence.</p></li>
<li><p><strong>Many more iterations needed</strong>: To converge with this learning rate would require several hundred iterations.</p></li>
</ol>
<p><strong>The learning rate is too small</strong>: Whilst smaller learning rates are more stable (less likely to diverge), if they’re too small, convergence becomes impractically slow. This learning rate of 0.001 is simply too conservative for efficient optimisation.</p>
</section>
<section id="part-3-learning-rate-0-1">
<h3>Part 3: Learning rate α = 0.1<a class="headerlink" href="#part-3-learning-rate-0-1" title="Link to this heading">#</a></h3>
<p>What happens with a larger learning rate?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run with α = 0.1 (larger)</span>
<span class="n">r_history_1</span><span class="p">,</span> <span class="n">grad_history_1</span><span class="p">,</span> <span class="n">converged_1</span><span class="p">,</span> <span class="n">iters_1</span> <span class="o">=</span> <span class="n">gradient_descent_adaptive</span><span class="p">(</span>
    <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r_0</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">50</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adaptive gradient descent with α = 0.1

Iteration | Position (Å) | Gradient (eV Å⁻¹) |     Δr (Å)
-----------------------------------------------------------------
        0 |     1.000000 |          9.360000 | -0.936000
        1 |     0.064000 |        -24.336000 |  2.433600
        2 |     2.497600 |         63.273600 | -6.327360
        3 |    -3.829760 |       -164.511360 | 16.451136
        4 |    12.621376 |        427.729536 | -42.772954
        5 |   -30.151578 |      -1112.096794 | 111.209679
        6 |    81.058102 |       2891.451663 | -289.145166
        7 |  -208.087065 |      -7517.774325 | 751.777432
        8 |   543.690368 |      19546.213244 | -1954.621324
        9 | -1410.930957 |     -50820.154435 | 5082.015444
       10 |  3671.084487 |     132132.401532 | -13213.240153
       11 | -9542.155666 |    -343544.243982 | 34354.424398
       12 | 24812.268732 |     893215.034353 | -89321.503435
       13 | -64509.234703 |   -2322359.089319 | 232235.908932
       14 | 167726.674229 |    6038133.632229 | -603813.363223
       15 | -436086.688994 |  -15699147.443794 | 1569914.744379
       16 | 1133828.055385 |   40817783.353866 | -4081778.335387
       17 | -2947950.280001 | -106126236.720050 | 10612623.672005
       18 | 7664673.392004 |  275928215.472131 | -27592821.547213
       19 | -19928148.155209 | -717413360.227540 | 71741336.022754
       20 | 51813187.867545 | 1865274736.591605 | -186527473.659161
       21 | -134714285.791616 | -4849714315.138174 | 484971431.513817
       22 | 350257145.722201 | 12609257219.359253 | -1260925721.935925
       23 | -910668576.213724 | -32784068770.334057 | 3278406877.033406
       24 | 2367738300.819682 | 85238578802.868561 | -8523857880.286857
       25 | -6156119579.467175 | -221620304887.458282 | 22162030488.745831
       26 | 16005910909.278656 | 576212792707.391602 | -57621279270.739166
       27 | -41615368361.460510 | -1498153261039.218262 | 149815326103.921844
       28 | 108199957742.461334 | 3895198478701.967773 | -389519847870.196777
       29 | -281319890127.735474 | -10127516044625.117188 | 1012751604462.511719
       30 | 731431714334.776245 | 26331541716025.304688 | -2633154171602.530762
       31 | -1901722457267.754395 | -68462008461665.796875 | 6846200846166.580078
       32 | 4944478388898.826172 | 178001222000331.093750 | -17800122200033.109375
       33 | -12855643811134.283203 | -462803177200860.875000 | 46280317720086.093750
       34 | 33424673908951.812500 | 1203288260722238.750000 | -120328826072223.875000
       35 | -86904152163272.062500 | -3128549477877820.500000 | 312854947787782.062500
       36 | 225950795624510.000000 | 8134228642482333.000000 | -813422864248233.375000
       37 | -587472068623723.375000 | -21148994470454068.000000 | 2114899447045407.000000
       38 | 1527427378421683.500000 | 54987385623180576.000000 | -5498738562318058.000000
       39 | -3971311183896374.500000 | -142967202620269504.000000 | 14296720262026952.000000
       40 | 10325409078130578.000000 | 371714726812700800.000000 | -37171472681270080.000000
       41 | -26846063603139504.000000 | -966458289713022208.000000 | 96645828971302224.000000
       42 | 69799765368162720.000000 | 2512791553253857792.000000 | -251279155325385792.000000
       43 | -181479389957223072.000000 | -6533258038460030976.000000 | 653325803846003072.000000
       44 | 471846413888780032.000000 | 16986470899996082176.000000 | -1698647089999608320.000000
       45 | -1226800676110828288.000000 | -44164824339989815296.000000 | 4416482433998981632.000000
       46 | 3189681757888153600.000000 | 114828543283973521408.000000 | -11482854328397352960.000000
       47 | -8293172570509199360.000000 | -298554212538331168768.000000 | 29855421253833117696.000000
       48 | 21562248683323916288.000000 | 776240952599660986368.000000 | -77624095259966095360.000000
       49 | -56061846576642179072.000000 | -2018226476759118512128.000000 | 201822647675911864320.000000

Did not converge within 50 iterations.
</pre></div>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<p>With <span class="math notranslate nohighlight">\(\alpha = 0.1\)</span>, the algorithm <strong>diverges</strong>! The position oscillates with increasing amplitude and never converges.</p>
<p>Why this happens:</p>
<ol class="arabic simple">
<li><p><strong>Overcorrection</strong>: The first step is:
$<span class="math notranslate nohighlight">\(\Delta r_0 = -\alpha U'(r_0) = -0.1 \times 9.36 = -0.936 \text{ Å}\)</span><span class="math notranslate nohighlight">\(
This moves from \)</span>r = 1.0<span class="math notranslate nohighlight">\( Å to \)</span>r = 0.064<span class="math notranslate nohighlight">\( Å, overshooting the minimum at \)</span>r_0 = 0.74$ Å by a large margin.</p></li>
<li><p><strong>Growing oscillations</strong>: Each subsequent step overshoots even more dramatically:</p>
<ul class="simple">
<li><p>The gradient on the far side of the minimum has opposite sign and large magnitude</p></li>
<li><p>The large <span class="math notranslate nohighlight">\(\alpha\)</span> amplifies this, causing increasingly wild swings</p></li>
<li><p>The position alternates sides but moves further from the minimum each time</p></li>
</ul>
</li>
<li><p><strong>Instability criterion</strong>: For the harmonic potential with <span class="math notranslate nohighlight">\(U'(r) = k(r - r_0)\)</span>, the algorithm is stable when:
$<span class="math notranslate nohighlight">\(\alpha &lt; \frac{2}{k}\)</span><span class="math notranslate nohighlight">\(
For our case with \)</span>k = 36.0<span class="math notranslate nohighlight">\( eV Å⁻²:
\)</span><span class="math notranslate nohighlight">\(\alpha &lt; \frac{2}{36} \approx 0.056\)</span><span class="math notranslate nohighlight">\(
Our \)</span>\alpha = 0.1$ exceeds this threshold, causing divergence.</p></li>
</ol>
<p>This demonstrates that whilst adaptive step sizes help, the learning rate parameter must still be chosen carefully. Too small and convergence is slow; too large and the algorithm diverges.</p>
</section>
<section id="summary-choosing-the-learning-rate">
<h3>Summary: Choosing the learning rate<a class="headerlink" href="#summary-choosing-the-learning-rate" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Adaptive vs. fixed</strong>: Adaptive step sizes (<span class="math notranslate nohighlight">\(r_{i+1} = r_i - \alpha U'(r_i)\)</span>) can outperform fixed step sizes because they automatically adjust based on proximity to the minimum.</p></li>
<li><p><strong>The learning rate determines convergence speed</strong>: There’s a “sweet spot” for <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<ul class="simple">
<li><p>Too small: stable but slow</p></li>
<li><p>Just right: fast convergence</p></li>
<li><p>Too large: divergence</p></li>
</ul>
</li>
<li><p><strong>Problem-specific tuning</strong>: The optimal <span class="math notranslate nohighlight">\(\alpha\)</span> depends on the problem (specifically, the curvature <span class="math notranslate nohighlight">\(k\)</span> for harmonic potentials). For different force constants, different learning rates would be optimal.</p></li>
</ol>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./worked_examples/week_5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="grid_search_method.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Worked Examples: Grid Search Method</p>
      </div>
    </a>
    <a class="right-next"
       href="newton_raphson_method.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Worked Examples: Newton-Raphson Method</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-fixed-step-size-gradient-descent">Exercise: Fixed Step Size Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-define-functions">Setup: Define functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-fixed-step-size-r-0-01-a">Part 1: Fixed step size Δ<em>r</em> = 0.01 Å</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-fixed-step-size-r-0-1-a">Part 2: Fixed step size Δr = 0.1 Å</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-oscillation">Visualizing the oscillation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-adaptive-step-size-gradient-descent">Exercise: Adaptive Step Size Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-learning-rate-0-01">Part 1: Learning rate α = 0.01</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-learning-rate-0-001">Part 2: Learning rate α = 0.001</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-learning-rate-0-1">Part 3: Learning rate α = 0.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-choosing-the-learning-rate">Summary: Choosing the learning rate</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Andrew R. McCluskey & Benjamin J. Morgan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>