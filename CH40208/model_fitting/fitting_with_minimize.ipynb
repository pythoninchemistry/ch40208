{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "source": [
    "## Linear Model Fitting with `scipy.optimize.minimize()`\n",
    "\n",
    "Having seen that $\\chi^2$ varies as we change our model parameters, and that it is smallest when the parameters are close to the true values, we now have a strategy for finding the best-fit parameters: we need to search through parameter space to find the values that minimise χ².\n",
    "\n",
    "This is an optimisation problem, and we can solve it using the same tool you encountered in Week 5 for geometry optimisation: `scipy.optimize.minimize()`. Just as you used `minimize()` to find the bond length that minimised potential energy, you can now use it to find the model parameters that minimise $\\chi^2$.\n",
    "\n",
    "In this section, you will implement the complete model fitting workflow:\n",
    "\n",
    "1. Define a model function\n",
    "2. Define an error function that calculates $\\chi^2$ for given parameters\n",
    "3. Use `minimize()` to find the best-fit parameters\n",
    "4. Visualise the results\n",
    "\n",
    "```{note}\n",
    "This section assumes you are continuing directly from the previous page and have the `x` and `y_data` arrays already defined. If you need to regenerate the data, refer back to the [previous page](understanding_residuals.ipynb).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-c9d0-1234-ef12-345678901234",
   "metadata": {},
   "source": [
    "### Defining the Model Function\n",
    "\n",
    "Before we can fit a model to our data, we need to define the mathematical form of our model. For a linear model, we want a function that calculates predicted $y$ values given input $x$ values and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9-d0e1-2345-f123-456789012345",
   "metadata": {},
   "source": [
    "#### Exercise: Write a Linear Model Function\n",
    "\n",
    "Write a function called `linear_model()` that implements the equation:\n",
    "\n",
    "$$y = mx + c$$\n",
    "\n",
    "Your function should:\n",
    "- Take three arguments: `x` (the independent variable), `m` (the slope), and `c` (the intercept)\n",
    "- Return the predicted y value(s)\n",
    "- Work with both single values and numpy arrays for `x`\n",
    "\n",
    "```python\n",
    "def linear_model(x, m, c):\n",
    "    \"\"\"Calculate y values for a linear model y = mx + c.\n",
    "    \n",
    "    Args:\n",
    "        x (float): Independent variable value(s).\n",
    "        m (float): Slope parameter.\n",
    "        c (float): Intercept parameter.\n",
    "        \n",
    "    Returns:\n",
    "        float: Predicted y value(s).\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "```\n",
    "\n",
    "Test your function by calculating and plotting `y_model = linear_model(x, 2.0, 3.0)` alongside your data points to verify it works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2g3-4567-2345-678901234567",
   "metadata": {},
   "source": [
    "### Defining the Error Function\n",
    "\n",
    "To use `scipy.optimize.minimize()`, we need an **error function** that:\n",
    "- Takes the parameters we want to optimise as its first argument\n",
    "- Takes the observed data as additional arguments\n",
    "- Returns a single number representing the quality of fit (which we want to minimise)\n",
    "\n",
    "For least-squares fitting, this error function should calculate $\\chi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2-g3h4-5678-3456-789012345678",
   "metadata": {},
   "source": [
    "#### Exercise: Write an Error Function\n",
    "\n",
    "Write a function called `error_function()` that calculates the sum-of-squares error.\n",
    "\n",
    "Your function should:\n",
    "- Take `params` as the first argument (a list or array containing `[m, c]`)\n",
    "- Take `x_data` and `y_data` as additional arguments\n",
    "- Extract the slope and intercept from `params`\n",
    "- Use your `linear_model()` function to calculate predicted $y$ values, given the input model slope and intercept\n",
    "- Calculate and return $\\chi^2$ as the sum of squared residuals\n",
    "\n",
    "```python\n",
    "def error_function(params, x_data, y_data):\n",
    "    \"\"\"Calculate sum-of-squares error for linear model.\n",
    "    \n",
    "    Args:\n",
    "        params (list): List/array containing [slope, intercept].\n",
    "        x_data (list): Observed x values.\n",
    "        y_data (list): Observed y values.\n",
    "        \n",
    "    Returns:\n",
    "        Chi-squared error value.\n",
    "    \"\"\"\n",
    "    m, c = params\n",
    "    # Your code here\n",
    "```\n",
    "\n",
    "Test your function by calculating the error for the true parameters `[2.0, 3.0]` and for some incorrect parameters like `[1.5, 4.0]`. The error should be smaller for the true parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4-i5j6-7890-5678-901234567890",
   "metadata": {},
   "source": [
    "### Finding the Best-Fit Parameters\n",
    "\n",
    "Now we have everything needed to perform the optimisation. We will use `scipy.optimize.minimize()` to find the parameter values that minimise our error function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2g3h4i5-j6k7-8901-6789-012345678901",
   "metadata": {},
   "source": [
    "#### Exercise: Optimise the Parameters\n",
    "\n",
    "Use `scipy.optimize.minimize()` to find the best-fit parameters.\n",
    "\n",
    "1. Import minimize:\n",
    "   ```python\n",
    "   from scipy.optimize import minimize\n",
    "   ```\n",
    "\n",
    "2. Choose initial guesses for the parameters. Try `initial_guess = [1.0, 1.0]` (deliberately quite different from the true values).\n",
    "\n",
    "3. Call `minimize`, passing:\n",
    "   - Your error function\n",
    "   - Your initial guess\n",
    "   - The observed data as additional arguments using `args=(x, y_data)`\n",
    "\n",
    "   ```python\n",
    "   result = minimize(error_function, initial_guess, args=(x, y_data))\n",
    "   ```\n",
    "\n",
    "4. Extract the optimised parameters from `result.x` and print them:\n",
    "   ```python\n",
    "   m_fit, c_fit = result.x\n",
    "   print(f\"Best-fit parameters: m = {m_fit:.4f}, c = {c_fit:.4f}\")\n",
    "   ```\n",
    "\n",
    "5. Print the final $\\chi^2$ value from `result.fun`.\n",
    "\n",
    "6. Check whether the optimisation was successful by printing `result.success`.\n",
    "\n",
    "```{margin}\n",
    "We have just found \"best-fit\" parameters by minimising $\\chi^2$. It's worth reflecting that this definition of \"best fit\" represents a choice we made. We could have defined our error function differently&mdash;for example, minimising the sum of absolute deviations or the maximum deviation&mdash;and obtained different \"best-fit\" parameters. Least-squares fitting (minimising $\\chi^2$) is a common and often well-motivated choice; it is particularly well-suited to data where the measurement errors are normally distributed, as they are here. However, recognising that \"best fit\" is defined by our choice of error function, rather than being an inherent property of the data, is important.\n",
    "```\n",
    "\n",
    "**Questions to consider:**\n",
    "- How close are the fitted parameters to the true values ($m$ = 2.0, $c$ = 3.0)?\n",
    "- Why are they not exactly the same?\n",
    "- How many iterations did the optimiser take? (Check `result.nit`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6-k7l8-9012-7890-123456789012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "h4i5j6k7-l8m9-0123-8901-234567890123",
   "metadata": {},
   "source": [
    "### Visualising the Fitted Model\n",
    "\n",
    "The final step in any model fitting workflow is to visualise how well the fitted model matches the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i5j6k7l8-m9n0-1234-9012-345678901234",
   "metadata": {},
   "source": [
    "#### Exercise: Plot the Data and Fitted Model\n",
    "\n",
    "Create a plot that shows:\n",
    "\n",
    "1. The measured data as scatter points with label \"Data\"\n",
    "\n",
    "2. The true model line (using $m$ = 2.0, $c$ = 3.0) as a dashed grey line with label \"True model\"\n",
    "\n",
    "3. The fitted model line (using your optimised parameters) as a solid line with label \"Fitted model\"\n",
    "\n",
    "4. Add a legend, axis labels, and a title.\n",
    "\n",
    "5. Optionally, add a text box showing the fitted parameters using:\n",
    "   ```python\n",
    "   plt.text(0.5, 20, f'Fitted: y = {m_fit:.3f}x + {c_fit:.3f}', fontsize=10)\n",
    "   ```\n",
    "\n",
    "The fitted line should lie very close to (or on top of) the true model line, confirming that our optimisation successfully recovered the underlying relationship from the noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j6k7l8m9-n0o1-2345-0123-456789012345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "k7l8m9n0-o1p2-3456-1234-567890123456",
   "metadata": {},
   "source": [
    "### Testing Different Initial Guesses\n",
    "\n",
    "A good optimisation algorithm should converge to the same minimum regardless of where it starts. Let us verify this for our linear fitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l8m9n0o1-p2q3-4567-2345-678901234567",
   "metadata": {},
   "source": [
    "#### Exercise: Explore Different Initial Guesses\n",
    "\n",
    "Repeat the optimisation with several different initial guesses:\n",
    "\n",
    "1. Create a list of initial guesses to test:\n",
    "   ```python\n",
    "   initial_guesses = [\n",
    "       [1.0, 1.0],\n",
    "       [0.0, 0.0],\n",
    "       [5.0, 10.0],\n",
    "       [-1.0, 8.0]\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "2. For each initial guess:\n",
    "   - Run the optimisation\n",
    "   - Print the initial guess, the optimised parameters, and the final $\\chi^2$\n",
    "\n",
    "3. Format your output clearly to compare the results.\n",
    "\n",
    "**Questions to consider:**\n",
    "- Do all starting points converge to approximately the same parameters?\n",
    "- Do all starting points give approximately the same final $\\chi^2$?\n",
    "- Does the number of iterations vary depending on the starting point?\n",
    "- Why is linear fitting generally more robust to initial guesses than some other optimisation problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m9n0o1p2-q3r4-5678-3456-789012345678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "n0o1p2q3-r4s5-6789-4567-890123456789",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "You have now completed the full workflow for model fitting:\n",
    "\n",
    "1. **Define your model** – A mathematical function that describes the relationship you expect\n",
    "2. **Define an error metric** – A function that quantifies how well parameters fit the data (typically $\\chi^2$)\n",
    "3. **Optimise the parameters** – Use `minimize()` to find parameters that minimise the error\n",
    "4. **Visualise the results** – Plot the data and fitted model to assess quality\n",
    "\n",
    "This same pattern applies to any model fitting problem, whether the model is linear or non-linear, and whether you have two parameters or twenty. The exercises that follow will give you practice applying this workflow to real chemical systems, where the models and data have physical significance.\n",
    "\n",
    "```{note}\n",
    "For linear regression specifically, there exist analytical solutions and specialized functions like `scipy.stats.linregress()` that are more efficient than using `minimize()`. However, the approach you have learned here is completely general and works for any model – including non-linear models where no analytical solution exists. This generality makes it a powerful tool for fitting diverse chemical models to experimental data.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34695923-06e9-493a-bce4-e7552605b88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
