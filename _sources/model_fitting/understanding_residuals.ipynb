{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "source": [
    "## Understanding Residuals and Quality of Fit\n",
    "\n",
    "Before applying model fitting to real experimental data, we need to understand what we mean by the \"quality of fit\" and how to quantify it. In this exercise, you will work with synthetic data where we know the true underlying relationship. This allows us to see clearly how residuals and the sum-of-squares error metric behave as we try different model parameters.\n",
    "\n",
    "### Generating Synthetic Data\n",
    "\n",
    "Let us start by creating some artificial data from a known linear relationship with added random noise. This simulates what happens in a real experiment: there is an underlying relationship between variables, but our measurements include some random experimental error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-bcde-f12345678901",
   "metadata": {},
   "source": [
    "#### Exercise: Generate Noisy Linear Data\n",
    "\n",
    "We will generate data from the linear relationship:\n",
    "\n",
    "$$y = 2.0x + 3.0$$\n",
    "\n",
    "and then add some random noise to simulate experimental error.\n",
    "\n",
    "1. Create an array `x` containing 20 evenly spaced values between 0 and 10 using `np.linspace()`.\n",
    "\n",
    "2. Calculate the \"true\" y values using the equation above. Store these in an array called `y_true`.\n",
    "\n",
    "3. Add random noise to create our \"measured\" data. Use the following code:\n",
    "```python\n",
    "   np.random.seed(42)\n",
    "   noise = np.random.normal(0, 1.5, size=20)\n",
    "   y_data = y_true + noise\n",
    "```\n",
    "```{margin}\n",
    "The line `np.random.seed(42)` ensures that everyone generates the same \"random\" numbers. Python's random number generator actually produces a deterministic sequence of numbers, but the sequence appears random. The seed value (42 here, though any integer works) determines which sequence is used. \n",
    "\n",
    "If you don't set a seed, NumPy initialises its random number generator using the current system time (or more precisely, using your operating system's source of randomness). This means you would get different random numbers every time you run your code, which is usually what you want in real research. However, for this exercise, setting the seed ensures your results will match the expected answers and makes it easier to check your work and discuss results with others.\n",
    "```   \n",
    "The function `np.random.normal(0, 1.5, size=20)` generates 20 random numbers drawn from a normal (Gaussian) distribution with mean 0 and standard deviation 1.5. These random values simulate the experimental errors that occur in real measurements.\n",
    "\n",
    "\n",
    "4. Create a plot showing:\n",
    "   - The true line (plot `x` vs `y_true` as a solid line)\n",
    "   - The noisy measured data (plot `x` vs `y_data` as scattered points)\n",
    "   - Appropriate axis labels and a legend\n",
    "\n",
    "This plot should show that whilst our measured data approximately follow the true linear relationship, individual points are scattered around the line due to random measurement error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-b8c9-0123-def1-234567890123",
   "metadata": {},
   "source": [
    "### Calculating Residuals\n",
    "\n",
    "The **residual** for each data point is the difference between the observed y value and the y value predicted by our model. If we have a perfect model with perfect parameters, and our data had no noise, all residuals would be zero. In practice, we want to find parameters that make the residuals as small as possible.\n",
    "\n",
    "For a linear model $y = mx + c$, the residual for the $i$-th data point is:\n",
    "\n",
    "$$r_i = y_i^\\mathrm{data} - y_i^\\mathrm{model} = y_i^\\mathrm{data} - (mx_i + c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-c9d0-1234-ef12-345678901234",
   "metadata": {},
   "source": [
    "#### Exercise: Calculate Residuals for the True Parameters\n",
    "\n",
    "Let us calculate the residuals when we use the true parameters $m = 2.0$ and $c = 3.0$.\n",
    "\n",
    "1. Calculate the predicted y values using the true parameters: `y_model = 2.0 * x + 3.0`\n",
    "\n",
    "2. Calculate the residuals: `residuals = y_data - y_model`\n",
    "\n",
    "3. Print the first 5 residuals to see what they look like.\n",
    "\n",
    "4. Create a plot showing:\n",
    "   - The model line (plot `x` vs `y_model`)\n",
    "   - The measured data points (plot `x` vs `y_data` as scatter)\n",
    "   - Vertical lines showing the residuals for each point using:\n",
    "     ```python\n",
    "     for i in range(len(x)):\n",
    "         plt.plot([x[i], x[i]], [y_data[i], y_model[i]], 'r-', alpha=0.5)\n",
    "     ```\n",
    "\n",
    "**Questions to consider:**\n",
    "- Are the residuals positive or negative? What does the sign tell you?\n",
    "- Are the residuals roughly the same size as the noise we added (standard deviation 1.5)?\n",
    "- Do the residuals appear randomly distributed around zero, or is there a pattern?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0-e1f2-3456-1234-567890123456",
   "metadata": {},
   "source": [
    "### Quantifying Quality of Fit with $\\chi^2$\n",
    "\n",
    "Whilst individual residuals tell us how well our model fits each point, we need a single number to quantify the overall quality of fit. The sum-of-squares error, $\\chi^2$, achieves this by adding up the squared residuals:\n",
    "\n",
    "$$\\chi^2 = \\sum_i r_i^2 = \\sum_i \\left[y_i^\\mathrm{data} - y_i^\\mathrm{model}\\right]^2$$\n",
    "\n",
    "Squaring the residuals ensures that positive and negative deviations both contribute positively to the error, and also gives more weight to larger deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2g3-4567-2345-678901234567",
   "metadata": {},
   "source": [
    "#### Exercise: Calculate $\\chi^2$ for the True Parameters\n",
    "\n",
    "1. Using the residuals you calculated above, compute $\\chi^2$ as the sum of squared residuals. You can use either:\n",
    "   - `chi_squared = np.sum(residuals**2)`\n",
    "   - `chi_squared = np.sum((y_data - y_model)**2)`\n",
    "\n",
    "2. Print the value of $\\chi^2$. This represents how well the true model parameters fit our noisy data.\n",
    "\n",
    "**Note:** Since we are using the true parameters that generated the data, this $\\chi^2$ value reflects only the random noise we added, not any systematic mismatch between model and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2g3-h4i5-6789-4567-890123456789",
   "metadata": {},
   "source": [
    "### Exploring How χ² Changes with Parameters\n",
    "\n",
    "$\\chi^2$ provides a single number that quantifies the overall quality of fit: smaller χ$\\chi^2$ values indicate that the model predictions are closer to the observed data. This suggests a natural way to think about what we mean by the \"best-fit\" parameters.\n",
    "\n",
    "If we want our model to fit the data as closely as possible, we should look for the parameters that make $\\chi^2$ as small as possible. This leads us to define the \"best-fit\" parameters as those that **minimise** $\\chi^2$.\n",
    "\n",
    "Let us verify that $\\chi^2$ does indeed vary as we change our parameters, and observe that it is smallest for parameters close to the true values that generated our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4-i5j6-7890-5678-901234567890",
   "metadata": {},
   "source": [
    "#### Exercise: Calculate $\\chi^2$ for Different Parameter Values\n",
    "\n",
    "We will test several different combinations of slope $m$ and intercept $c$ to see how $\\chi^2$ varies.\n",
    "\n",
    "1. Create a list of parameter pairs to test:\n",
    "   ```python\n",
    "   parameters_to_test = [\n",
    "       (2.0, 3.0),   # True parameters\n",
    "       (1.5, 3.0),   # Wrong slope\n",
    "       (2.0, 4.0),   # Wrong intercept  \n",
    "       (2.5, 2.0),   # Both wrong\n",
    "       (1.0, 5.0),   # Both very wrong\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "2. For each parameter pair `(m, c)` in this list:\n",
    "   - Calculate the predicted y values: `y_model = m * x + c`\n",
    "   - Calculate $\\chi^2$: `chi_squared = np.sum((y_data - y_model)**2)`\n",
    "   - Print the parameters and the resulting $\\chi^2$ value\n",
    "\n",
    "3. Format your output neatly, for example:\n",
    "   ```\n",
    "   m = 2.0, c = 3.0: χ² = ...\n",
    "   m = 1.5, c = 3.0: χ² = ...\n",
    "   ```\n",
    "\n",
    "**Questions to consider:**\n",
    "- Which parameter combination gives the smallest $\\chi^2$?\n",
    "- How much does $\\chi^2$ increase when you use the wrong parameters?\n",
    "- Does $\\chi^2$ increase more when the slope is wrong or when the intercept is wrong? Can you explain why, based on your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g3h4i5j6-k7l8-9012-7890-123456789012",
   "metadata": {},
   "source": [
    "#### Exercise: Visualise Different Fits\n",
    "\n",
    "Create a single plot that compares how well different parameter choices fit the data.\n",
    "\n",
    "On one set of axes, plot:\n",
    "   - The measured data as scatter points with label \"Data\"\n",
    "   - Three model lines using different parameters from your list above (for example: the true parameters, slightly wrong parameters, and very wrong parameters)\n",
    "   - Use different line styles or colours for each model line\n",
    "   - Add labels indicating each line's parameters and $\\chi^2$ value\n",
    "   - Include a legend\n",
    "\n",
    "This visualisation should make clear the relationship between how well a model fits the data (visually) and the corresponding $\\chi^2$ value. Parameters that give lines closer to the data points, in general, result in smaller $\\chi^2$ values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
