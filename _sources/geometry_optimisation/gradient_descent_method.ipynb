{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1f9693-959a-488c-b0fd-93da069a3fd2",
   "metadata": {},
   "source": [
    "### Gradient Descent Method\n",
    "\n",
    "A more efficient approach is to use information about the slope of the potential energy surface to guide our search. \n",
    "\n",
    "The **gradient descent method** (also called the **steepest descent method**) works by analogy to releasing a ball on a hill and letting it roll to the bottom. At any point on our potential energy surface, the **gradient** tells us which direction is \"uphill\" and how steep the surface is at this point. With this information, we can step in the opposite direction (i.e., downhill), then recalculate the gradient at our new position, and repeat until we reach a point where the gradient is $\\sim0$.\n",
    "\n",
    "The simplest implementation of this method is to move a fixed distance every step.\n",
    "\n",
    "#### Algorithm (Fixed Step Size)\n",
    "\n",
    "1. Start at an initial guess position $r_0$\n",
    "2. Calculate the gradient $U^\\prime(r) = \\frac{\\mathrm{d}U}{\\mathrm{d}r}$ at this point\n",
    "3. Move in the direction opposite to the gradient (i.e., downhill):\n",
    "   $$r_{i+1} = r_i - \\Delta r \\times \\mathrm{sign}(U^\\prime(r_i))$$\n",
    "   where $\\Delta r$ is a fixed step size and $\\mathrm{sign}(U^\\prime)$ gives us the direction (+1 or -1)\n",
    "4. Repeat until the gradient is close to zero\n",
    "```{note}\n",
    "In molecular simulations, we often express the gradient of the potential energy in terms of the **force**. The force is defined as the negative gradient of the potential energy:\n",
    "\n",
    "$$F(r) = -\\frac{\\mathrm{d}U}{\\mathrm{d}r} = -U^\\prime(r)$$\n",
    "\n",
    "This relationship means that forces naturally point \"downhill\" towards lower energy, whilst the gradient points \"uphill\" towards higher energy. When particles move in the direction of the force, they move towards lower potential energy.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd899cfa-e2eb-4a33-b0c2-20860f6635cf",
   "metadata": {},
   "source": [
    "#### Exercise: Fixed Step Size Gradient Descent\n",
    "\n",
    "Write a function to calculate the first derivative of a harmonic potential:\n",
    "\n",
    "$$U^\\prime(r) = k(r - r_0)$$\n",
    "\n",
    "Using this function, write code to perform a gradient descent search to find the minimum of your harmonic potential energy surface. Use $r=1.0$ Å as your starting position.\n",
    "\n",
    "Your code should:\n",
    "- Use a `for` loop with a **maximum of 50 iterations** to prevent infinite loops\n",
    "- Store the position and gradient at each iteration in lists (for plotting later)\n",
    "- Update the position at each step using: $r_{i+1} = r_i - \\Delta r \\times \\mathrm{sign}(U^\\prime(r_i))$\n",
    "- Print the iteration number, position, and gradient at each step\n",
    "- Stop early (using `break`) when $|U^\\prime(r)| < 0.001$\n",
    "- Report whether the algorithm converged or hit the maximum iteration limit\n",
    "\n",
    "**Part 1:** Start with $\\Delta r = 0.01$ Å. Does the algorithm converge? How many iterations does it require?\n",
    "\n",
    "**Part 2:** Now try $\\Delta r = 0.1$ Å to see if a larger step size converges faster. What behaviour do you observe? Does it still converge?\n",
    "\n",
    "**Questions to consider:**\n",
    "- Why does the larger step size cause oscillation?\n",
    "- The minimum is at r = 0.74 Å and we start at r = 1.0 Å. Can you explain why Δr = 0.01 Å converges but Δr = 0.1 Å does not, based on this distance?\n",
    "- What would you predict for Δr = 0.05 Å?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e7a51-e884-45d7-9c60-828fc80f61bd",
   "metadata": {},
   "source": [
    "#### Rescaling the Step Size\n",
    "\n",
    "Using a fixed step size makes our method very sensitive to the choice of step size. As you have seen, large step sizes overshoot the minimum and oscillate back and forth, whilst small step sizes converge more reliably but require many more iterations.\n",
    "\n",
    "A common approach designed to address this problem is to rescale the step size for each iteration based on how far we think we are from the minimum. A simple model assumes that the gradient will be steep if we are far from the minimum, but shallow if we are already close. Therefore, we make our step size proportional to the local gradient magnitude.\n",
    "\n",
    "Instead of moving a fixed distance $\\Delta r$, we take a step proportional to the gradient:\n",
    "\n",
    "$$\\Delta r_i = -\\alpha U^\\prime(r_i)$$\n",
    "\n",
    "or equivalently, proportional to the force:\n",
    "\n",
    "$$\\Delta r_i = \\alpha F(r_i)$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is a constant called the **learning rate** or **step size parameter**\n",
    "- $F(r_i) = -U^\\prime(r_i)$ is the force at position $r_i$\n",
    "\n",
    "The update equation then becomes:\n",
    "\n",
    "$$r_{i+1} = r_i - \\alpha U^\\prime(r_i)$$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$$r_{i+1} = r_i + \\alpha F(r_i)$$\n",
    "\n",
    "With this adaptive approach:\n",
    "- When far from the minimum, $|U^\\prime|$ is large → we take large steps\n",
    "- When close to the minimum, $|U^\\prime|$ is small → we take small steps\n",
    "\n",
    "The parameter $\\alpha$ controls the overall \"aggressiveness\" of the optimisation. Too large and we overshoot; too small and we converge slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84df851-bdbd-48d5-99a1-da0df6ea6034",
   "metadata": {},
   "source": [
    "#### Exercise: Adaptive Step Size Gradient Descent\n",
    "\n",
    "1. Write a new version of your gradient descent code that rescales the step to be proportional to the local force. You should write this as a loop that iteratively updates the current position:\n",
    "\n",
    "$$r_{i+1} = r_i + \\alpha F(r_i)$$\n",
    "\n",
    "where $F(r_i) = -U^\\prime(r_i) = -k(r_i - r_0)$.\n",
    "\n",
    "Your code should:\n",
    "- Start from $r = 1.0$ Å\n",
    "- Use a `for` loop with a **maximum of 50 iterations**\n",
    "- Update the position at each iteration\n",
    "- Print the iteration number, position, gradient, and step size taken at each iteration\n",
    "- Stop early (using `break`) when $|U^\\prime(r)| < 0.001$\n",
    "- Report the final predicted equilibrium bond length and number of iterations required\n",
    "\n",
    "**Part 1:** Start with $\\alpha = 0.01$. Does it converge? How many iterations does it take? Compare this to the fixed step size results.\n",
    "\n",
    "**Part 2:** Try $\\alpha = 0.001$. What happens to the convergence rate?\n",
    "\n",
    "**Part 3:** Try $\\alpha = 0.1$. Does the algorithm remain stable?\n",
    "\n",
    "**Questions to consider:**\n",
    "- How does the step size change as you approach the minimum? Watch the \"Step (Å)\" column in your output.\n",
    "- Why doesn't the adaptive method oscillate like the fixed step size method with $\\Delta r = 0.1$ Å did?\n",
    "- Compare $\\alpha = 0.01$ (adaptive) with $\\Delta r = 0.01$ Å (fixed): which converges faster and why?\n",
    "- What happens when $\\alpha$ is too large? Can you explain this behaviour?\n",
    "- Based on your experiments, what seems to be a good choice of $\\alpha$ for this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100979f-a7ce-4818-b2fa-43bf4848966b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
