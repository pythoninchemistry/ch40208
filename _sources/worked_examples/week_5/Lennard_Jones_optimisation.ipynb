{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf6622e-b076-4d4d-81b7-8f3cf4cca200",
   "metadata": {},
   "source": [
    "# Worked Examples: Lennard-Jones Optimisation\n",
    "\n",
    "These worked solutions correspond to the [Synoptic Exercise: Geometry Optimisation of a Lennard-Jones Potential](../../geometry_optimisation/Lennard_Jones_optimisation.ipynb).\n",
    "\n",
    "**How to use this notebook:**\n",
    "- Try each exercise yourself first before looking at the solution\n",
    "- The code cells show both the code and its output\n",
    "- Download this notebook to run and experiment with the code yourself\n",
    "- Your solution might look different - that's fine as long as it gives the correct answer!\n",
    "\n",
    "**Parameters for this exercise:**\n",
    "- $A$ = 1 × 10<sup>5</sup> eV Å<sup>12</sup>\n",
    "- $B$ = 40 eV Å<sup>6</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3a7b2-887f-4819-bdb6-a07ced477b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell: Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Define constants\n",
    "A = 1e5  # eV Å^12\n",
    "B = 40   # eV Å^6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74e01c-0774-4b0f-970b-18626b2a5c2d",
   "metadata": {},
   "source": [
    "## Part A: Visualising the Potential Energy Surface\n",
    "\n",
    "This section helps us understand the shape of the Lennard-Jones potential before attempting optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31203f53-79cc-4bf5-89a4-d7cf3edafb75",
   "metadata": {},
   "source": [
    "### Question 1: Define the Lennard-Jones function\n",
    "\n",
    "Write a function `lennard_jones(r, A, B)` that calculates $U_\\mathrm{LJ}(r) = \\frac{A}{r^{12}} - \\frac{B}{r^6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b7ded-3fd3-487b-bb11-9547434a03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lennard_jones(r, A, B):\n",
    "    \"\"\"\n",
    "    Calculate the Lennard-Jones potential energy.\n",
    "    \n",
    "    Args:\n",
    "        r (float): Interatomic separation in Angstroms.\n",
    "        A (float): Repulsive term coefficient (eV Å^12).\n",
    "        B (float): Attractive term coefficient (eV Å^6).\n",
    "        \n",
    "    Returns:\n",
    "        float: Potential energy in eV.\n",
    "    \"\"\"\n",
    "    return A / r**12 - B / r**6\n",
    "\n",
    "# Test the function at r = 4.4 Å\n",
    "test_r = 4.4\n",
    "test_energy = lennard_jones(test_r, A, B)\n",
    "print(f\"At r = {test_r} Å, U = {test_energy:.4f} eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c89c2c-f69a-411c-ab35-6fc053a2660d",
   "metadata": {},
   "source": [
    "### Questions 2-3: Create array and plot\n",
    "\n",
    "Create an array of $r$ values from 3.6 Å to 8.0 Å and plot the potential energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e0a56-e494-491d-85c1-3bd6d18059fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of r values\n",
    "r_values = np.linspace(3.6, 8.0, 200)\n",
    "\n",
    "# Calculate potential energy at each point\n",
    "U_values = lennard_jones(r_values, A, B)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(r_values, U_values, '-')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3, label='U = 0')\n",
    "plt.xlabel('Interatomic separation, $r$ / Å')\n",
    "plt.ylabel('Potential energy, $U$ / eV')\n",
    "plt.title('Lennard-Jones Potential Energy Surface')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739cf5a-ec24-409c-af51-21bf58c5943d",
   "metadata": {},
   "source": [
    "### Question 4: Analyse the plot\n",
    "\n",
    "By examining the plot, we can identify key features of the potential energy surface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fcdeba-6bbd-432a-b43d-8974d89bacac",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0e5d1-b2d9-4a8d-859b-40fca3e35475",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx = np.argmin(U_values)\n",
    "r_min = r_values[min_idx]\n",
    "U_min = U_values[min_idx]\n",
    "\n",
    "# Find where U crosses zero (approximately)\n",
    "# Look for sign change from negative to positive\n",
    "zero_crossing_idx = np.where(np.diff(np.sign(U_values)))[0]\n",
    "if len(zero_crossing_idx) > 0:\n",
    "    r_zero = r_values[zero_crossing_idx[0]]\n",
    "else:\n",
    "    r_zero = None\n",
    "\n",
    "print(\"Analysis of the Lennard-Jones potential:\")\n",
    "print(f\"- Minimum location (equilibrium bond length): r ≈ {r_min:.2f} Å\")\n",
    "print(f\"- Energy at minimum: U ≈ {U_min:.3f} eV\")\n",
    "if r_zero is not None:\n",
    "    print(f\"- Zero crossing (U = 0): r ≈ {r_zero:.2f} Å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533dad3-3b6c-4cc5-b748-158470351913",
   "metadata": {},
   "source": [
    "**Physical interpretation:**\n",
    "\n",
    "- The **minimum** at $r$ ≈ 4.13 Å represents the equilibrium separation where the forces balance ($\\mathrm{d}U/\\mathrm{d}r = 0$).\n",
    "- The **negative energy** at the minimum indicates a bound state (the atoms attract each other).\n",
    "- The **zero crossing** at $r$ ≈ 3.67 Å represents where the repulsive term ($A/r^{12}$) exactly equals the attractive term ($B/r^6$). At distances shorter than this, the net potential is repulsive ($U > 0$); at distances longer than this, the net potential is attractive ($U < 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1323b1-13c6-4d3a-bba6-57c24b422721",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B: Gradient Descent Optimisation\n",
    "\n",
    "Now we'll use gradient descent with an adaptive step size to find the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3098489-f408-47e6-b0a3-b429d653a7d2",
   "metadata": {},
   "source": [
    "### Question 5: Define the gradient function\n",
    "\n",
    "Write a function `lennard_jones_gradient(r, A, B)` that calculates $U'_\\mathrm{LJ}(r) = -12\\frac{A}{r^{13}} + 6\\frac{B}{r^7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb08502-d07c-424b-8f03-f353bf123d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lennard_jones_gradient(r, A, B):\n",
    "    \"\"\"\n",
    "    Calculate the first derivative of the Lennard-Jones potential.\n",
    "    \n",
    "    Args:\n",
    "        r (float): Interatomic separation in Angstroms \n",
    "        A (float): Repulsive term coefficient (eV Å^12).\n",
    "        B (flaot): Attractive term coefficient (eV Å^6).\n",
    "        \n",
    "    Returns:\n",
    "        First derivative dU/dr in eV/Å.\n",
    "    \"\"\"\n",
    "    return -12 * A / r**13 + 6 * B / r**7\n",
    "\n",
    "# Test the gradient function at r = 4.1 Å\n",
    "test_gradient = lennard_jones_gradient(test_r, A, B)\n",
    "print(f\"At r = {test_r} Å, U' = {test_gradient:.6f} eV/Å\")\n",
    "print(\"(Should be close to zero since r ≈ 4.1 Å is near the minimum)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289b10c-a0a4-4250-8521-d912a9243bff",
   "metadata": {},
   "source": [
    "### Questions 6-8: Implement gradient descent\n",
    "\n",
    "Implement a gradient descent loop starting from r = 5.0 Å with learning rate α = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d7b96-fbf8-47c6-9d29-9b7924890593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent parameters\n",
    "r_start = 5.0  # Å\n",
    "alpha = 100    # Learning rate\n",
    "max_iterations = 50\n",
    "tolerance = 0.001\n",
    "\n",
    "# Initialize storage lists\n",
    "positions = [r_start]\n",
    "gradients = [lennard_jones_gradient(r_start, A, B)]\n",
    "\n",
    "# Current position\n",
    "r_current = r_start\n",
    "\n",
    "# Gradient descent loop\n",
    "for iteration in range(max_iterations):\n",
    "    # Calculate gradient at current position\n",
    "    grad = lennard_jones_gradient(r_current, A, B)\n",
    "    \n",
    "    # Update position using: r_{i+1} = r_i - alpha * U'(r_i)\n",
    "    r_new = r_current - alpha * grad\n",
    "    \n",
    "    # Store the new position and gradient\n",
    "    positions.append(r_new)\n",
    "    gradients.append(lennard_jones_gradient(r_new, A, B))\n",
    "    \n",
    "    # Check convergence\n",
    "    if abs(gradients[-1]) < tolerance:\n",
    "        print(f\"Converged after {iteration + 1} iterations\")\n",
    "        break\n",
    "    \n",
    "    # Update current position\n",
    "    r_current = r_new\n",
    "else:\n",
    "    print(f\"Reached maximum iterations ({max_iterations})\")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nFinal predicted equilibrium separation: r = {positions[-1]:.4f} Å\")\n",
    "print(f\"Number of iterations required: {len(positions) - 1}\")\n",
    "print(f\"Final gradient value: {gradients[-1]:.6f} eV/Å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3017a1-edf1-4735-9498-da10c09d471c",
   "metadata": {},
   "source": [
    "### Visualise the convergence path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18474c-e9ce-40e5-b17a-7a1ae9ae728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the full potential energy surface\n",
    "plt.plot(r_values, U_values, '-', label='LJ Potential')\n",
    "\n",
    "# Calculate energies at visited positions\n",
    "energies_visited = [lennard_jones(r, A, B) for r in positions]\n",
    "\n",
    "# Plot the convergence path\n",
    "plt.plot(positions, energies_visited, 'o-', markersize=8, \n",
    "         label='Gradient descent path')\n",
    "\n",
    "# Mark the starting point\n",
    "plt.plot(positions[0], energies_visited[0], 'gs', markersize=8, \n",
    "         label=f'Start (r = {positions[0]:.1f} Å)')\n",
    "\n",
    "# Mark the final point\n",
    "plt.plot(positions[-1], energies_visited[-1], 'rs', markersize=8, \n",
    "         label=f'Final (r = {positions[-1]:.2f} Å)')\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Interatomic separation, $r$ / Å')\n",
    "plt.ylabel('Potential energy, $U$ / eV')\n",
    "plt.title('Gradient Descent Convergence on LJ Potential Surface')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Starting position: r = {positions[0]:.2f} Å, U = {energies_visited[0]:.3f} eV\")\n",
    "print(f\"Final position: r = {positions[-1]:.2f} Å, U = {energies_visited[-1]:.3f} eV\")\n",
    "print(f\"Energy change: ΔU = {energies_visited[-1] - energies_visited[0]:.3f} eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c8898-deda-43fd-af41-1d630bd207b3",
   "metadata": {},
   "source": [
    "**Analysis of convergence behaviour:**\n",
    "\n",
    "- **Number of iterations**: The algorithm converged in 4 iterations from this starting position (r = 5.0 Å).\n",
    "- **Convergence smoothness**: The convergence is smooth and monotonic - the algorithm follows a clear path down the potential surface with no oscillation.\n",
    "- **Adaptive step size**: The step size automatically decreases as we approach the minimum. At r = 5.0 Å, the gradient has magnitude |U'| ≈ 0.003 eV/Å, giving an initial step size of about 0.3 Å. As we approach the minimum where |U'(r)| → 0, the step size α × |U'(r)| naturally decreases, preventing overshooting and enabling smooth convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e9c19-fa91-4dbf-b49b-8452791d51bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C: Exploring Different Starting Positions with Gradient Descent\n",
    "\n",
    "The choice of starting position can significantly affect optimisation performance. We'll test three different starting positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb30cd-a00f-4275-90b3-fcc8079398de",
   "metadata": {},
   "source": [
    "### Questions 9-10: Test multiple starting positions\n",
    "\n",
    "Test gradient descent with three different starting positions and compare the results.\n",
    "\n",
    "Note the change in recommended parameters to $\\alpha$ = 10, `tolerance` = 0.0005, and `max_iterations` = 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a7f10-c2d8-4522-8f14-e6a111b5b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test three different starting positions\n",
    "starting_positions = [3.2, 4.4, 6.0]  # Å\n",
    "alpha = 10\n",
    "max_iterations = 500\n",
    "tolerance = 0.0005\n",
    "\n",
    "# Store results for each starting position\n",
    "results = []\n",
    "\n",
    "for r_start in starting_positions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting from r = {r_start} Å\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Initialize\n",
    "    positions_trial = [r_start]\n",
    "    gradients_trial = [lennard_jones_gradient(r_start, A, B)]\n",
    "    r_current = r_start\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    converged = False\n",
    "    for iteration in range(max_iterations):\n",
    "        grad = lennard_jones_gradient(r_current, A, B)\n",
    "        r_new = r_current - alpha * grad\n",
    "        \n",
    "        positions_trial.append(r_new)\n",
    "        gradients_trial.append(lennard_jones_gradient(r_new, A, B))\n",
    "        \n",
    "        if abs(gradients_trial[-1]) < tolerance:\n",
    "            converged = True\n",
    "            print(f\"✓ Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        r_current = r_new\n",
    "    \n",
    "    if not converged:\n",
    "        print(f\"✗ Did NOT converge within {max_iterations} iterations\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'r_start': r_start,\n",
    "        'r_final': positions_trial[-1],\n",
    "        'iterations': len(positions_trial) - 1,\n",
    "        'converged': converged,\n",
    "        'positions': positions_trial,\n",
    "        'gradients': gradients_trial\n",
    "    })\n",
    "    \n",
    "    print(f\"Final position: r = {positions_trial[-1]:.4f} Å\")\n",
    "    print(f\"Final gradient: |U'| = {abs(gradients_trial[-1]):.6f} eV/Å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae8125-e7e9-4e60-b671-86c9607ca7c7",
   "metadata": {},
   "source": [
    "**Analysis of results:**\n",
    "\n",
    "**Do all starting positions converge to the same minimum?**\n",
    "\n",
    "Yes, all three starting positions successfully converge to essentially the same minimum at r ≈ 4.16-4.17 Å, which matches our expectation from Part A. This demonstrates that gradient descent reliably finds the global minimum of the Lennard-Jones potential when appropriate parameters are used.\n",
    "\n",
    "**Which starting position converges most quickly?**\n",
    "\n",
    "Starting from r = 4.4 Å converges most quickly (16 iterations) because it's already close to the minimum. Starting from r = 3.2 Å requires 96 iterations, whilst r = 6.0 Å requires the most (123 iterations). The large number of iterations for r = 6.0 Å occurs because the gradient in this region is very small, so each step is modest in size.\n",
    "\n",
    "**Did any starting positions fail to converge?**\n",
    "\n",
    "With these parameters, all positions converge successfully. However, the need to increase max_iterations from 50 to 1000 and reduce α from 100 to 10 reveals important limitations of gradient descent with fixed learning rates.\n",
    "\n",
    "**Slow convergence from distant starting points:**\n",
    "\n",
    "The 96-123 iterations required when starting far from the minimum (r = 3.2 Å and r = 6.0 Å) highlight a key weakness of gradient descent: convergence can be very slow when the starting position is far from the optimum. This is particularly problematic for complex molecular systems where:\n",
    "\n",
    "1. We often have poor initial guesses for the geometry\n",
    "2. High-dimensional systems (molecules with many atoms) may require even more iterations\n",
    "3. Each iteration requires expensive energy and gradient calculations\n",
    "\n",
    "This inefficiency motivates the need for more sophisticated optimisation algorithms that converge faster, such as the Newton-Raphson method we'll explore in Part D.\n",
    "\n",
    "**Is α = 10 appropriate everywhere?**\n",
    "\n",
    "The varying iteration counts reveal that no single α value is optimal across all regions:\n",
    "\n",
    "- **At r = 6.0 Å**: The gradient is very small (|U'| ~ 0.001 eV/Å), so α = 10 gives tiny steps (~0.01 Å), requiring 123 iterations. A larger α would converge faster here.\n",
    "\n",
    "- **Near the minimum (r ≈ 4.4 Å)**: The gradient is nearly zero, so α = 10 works well - steps automatically become small and convergence is smooth (16 iterations).\n",
    "\n",
    "- **At r = 3.2 Å (repulsive wall)**: The gradient is large (|U'| ~ 0.1-1 eV/Å), so even with α = 10, the first step overshoots the minimum significantly, pushing the system into the long-range attractive region (r > 6 Å). The algorithm then spends most of the 96 iterations slowly crawling back down from this shallow-gradient region. This demonstrates the challenge of choosing α: a value that prevents catastrophic overshooting (like α = 100 would cause) can still result in initial overshooting followed by very slow convergence.\n",
    "\n",
    "**Key insights:**\n",
    "\n",
    "This exercise demonstrates a fundamental challenge with fixed-step gradient descent: the optimal learning rate varies dramatically across the potential energy surface. The Lennard-Jones potential is particularly challenging because:\n",
    "\n",
    "1. The **repulsive wall** (r < 4 Å) has very steep gradients requiring small α\n",
    "2. The **long-range region** (r > 5 Å) has very shallow gradients that could benefit from large α\n",
    "3. No single α value is optimal everywhere\n",
    "\n",
    "More sophisticated optimisation methods address this through adaptive learning rates, line search algorithms, or by using second-order derivative information (as in Newton-Raphson, which we explore next)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c21a96-0518-45f3-b3cd-1079d6066661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part D: Newton-Raphson Optimisation\n",
    "\n",
    "The Newton-Raphson method uses both first and second derivative information to converge more rapidly than gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fe6e9-749b-4b23-afc0-17416c5ff5ad",
   "metadata": {},
   "source": [
    "### Question 11: Define the second derivative function\n",
    "\n",
    "Write a function `lennard_jones_second_derivative(r, A, B)` that calculates $U''_\\mathrm{LJ}(r) = 156\\frac{A}{r^{14}} - 42\\frac{B}{r^8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861fbc0-9ebf-47a1-a57f-875f61f2dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lennard_jones_second_derivative(r, A, B):\n",
    "    \"\"\"\n",
    "    Calculate the second derivative of the Lennard-Jones potential.\n",
    "    \n",
    "    Args:\n",
    "        r (float): Interatomic separation in Angstroms.\n",
    "        A (float): Repulsive term coefficient (eV Å^12).\n",
    "        B (float): Attractive term coefficient (eV Å^6).\n",
    "        \n",
    "    Returns:\n",
    "        flaot: Second derivative d²U/dr² in eV/Å².\n",
    "    \"\"\"\n",
    "    return 156 * A / r**14 - 42 * B / r**8\n",
    "\n",
    "# Test the function at r = 4.4 Å (near the minimum)\n",
    "test_r = 4.4\n",
    "test_second_deriv = lennard_jones_second_derivative(test_r, A, B)\n",
    "print(f\"At r = {test_r} Å:\")\n",
    "print(f\"  U' = {lennard_jones_gradient(test_r, A, B):.6f} eV/Å\")\n",
    "print(f\"  U'' = {test_second_deriv:.6f} eV/Å²\")\n",
    "print(\"\\nAt a minimum, U' ≈ 0 and U'' > 0 (positive curvature)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5e6c4-99f5-422b-a96d-28f7efb350cd",
   "metadata": {},
   "source": [
    "### Questions 12-13: Implement Newton-Raphson optimisation\n",
    "\n",
    "Implement Newton-Raphson starting from r = 5.0 Å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b08c8-f220-484e-bba5-28e0ec654ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton-Raphson parameters\n",
    "r_start_nr = 5.0  # Å\n",
    "max_iterations = 50\n",
    "tolerance = 0.0005  # Using same tolerance as gradient descent for comparison\n",
    "\n",
    "# Initialize storage\n",
    "positions_nr = [r_start_nr]\n",
    "gradients_nr = [lennard_jones_gradient(r_start_nr, A, B)]\n",
    "\n",
    "# Current position\n",
    "r_current = r_start_nr\n",
    "\n",
    "# Newton-Raphson loop\n",
    "for iteration in range(max_iterations):\n",
    "    # Calculate first and second derivatives\n",
    "    grad = lennard_jones_gradient(r_current, A, B)\n",
    "    second_deriv = lennard_jones_second_derivative(r_current, A, B)\n",
    "    \n",
    "    # Update using Newton-Raphson: r_{i+1} = r_i - U'(r_i) / U''(r_i)\n",
    "    r_new = r_current - grad / second_deriv\n",
    "    \n",
    "    # Store position and gradient\n",
    "    positions_nr.append(r_new)\n",
    "    gradients_nr.append(lennard_jones_gradient(r_new, A, B))\n",
    "    \n",
    "    # Check convergence\n",
    "    if abs(gradients_nr[-1]) < tolerance:\n",
    "        print(f\"Converged after {iteration + 1} iterations\")\n",
    "        break\n",
    "    \n",
    "    # Update current position\n",
    "    r_current = r_new\n",
    "else:\n",
    "    print(f\"Did not converge within {max_iterations} iterations\")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nFinal equilibrium separation: r = {positions_nr[-1]:.4f} Å\")\n",
    "print(f\"Number of iterations required: {len(positions_nr) - 1}\")\n",
    "print(f\"Final gradient: |U'| = {abs(gradients_nr[-1]):.6f} eV/Å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736bfe4f-4754-40a3-a030-811c76506ba0",
   "metadata": {},
   "source": [
    "### Question 14: Compare convergence paths\n",
    "\n",
    "Compare the convergence of gradient descent and Newton-Raphson, both starting from r = 5.0 Å.\n",
    "\n",
    "**Note:** We need to re-run gradient descent from r = 5.0 Å with parameters ($\\alpha$ = 10, `tolerance` = 0.0005) for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e7cdf-f634-4bc0-ac23-5545a0b2ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run gradient descent from r = 5.0 Å for comparison\n",
    "r_start_gd = 5.0\n",
    "alpha = 10\n",
    "positions_gd = [r_start_gd]\n",
    "gradients_gd = [lennard_jones_gradient(r_start_gd, A, B)]\n",
    "r_current = r_start_gd\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    grad = lennard_jones_gradient(r_current, A, B)\n",
    "    r_new = r_current - alpha * grad\n",
    "    \n",
    "    positions_gd.append(r_new)\n",
    "    gradients_gd.append(lennard_jones_gradient(r_new, A, B))\n",
    "    \n",
    "    if abs(gradients_gd[-1]) < tolerance:\n",
    "        break\n",
    "    \n",
    "    r_current = r_new\n",
    "\n",
    "print(f\"Gradient descent from r = 5.0 Å: {len(positions_gd) - 1} iterations\")\n",
    "print(f\"Newton-Raphson from r = 5.0 Å: {len(positions_nr) - 1} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6a71a-12a7-4a4d-8b47-0b532efb6e68",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "**How does the convergence speed of Newton-Raphson compare to gradient descent?**\n",
    "\n",
    "Newton-Raphson converges dramatically faster than gradient descent. From r = 5.0 Å, Newton-Raphson requires only 2 iterations, whilst gradient descent requires 41 iterations - more than 20 times as many! This superior convergence is due to Newton-Raphson using second-derivative (curvature) information to take more accurate steps towards the minimum.\n",
    "\n",
    "**Why doesn't Newton-Raphson converge in a single step?**\n",
    "\n",
    "Unlike the harmonic potential (where Newton-Raphson converges in exactly one step), the Lennard-Jones potential is not quadratic. Newton-Raphson assumes the function is locally quadratic around the current position, but:\n",
    "\n",
    "- The harmonic potential is **exactly** quadratic everywhere, so the local quadratic approximation is perfect\n",
    "- The Lennard-Jones potential is **approximately** quadratic only near the minimum\n",
    "- Far from the minimum, the LJ potential has significant higher-order (cubic, quartic, etc.) terms that the quadratic approximation doesn't capture\n",
    "\n",
    "Therefore, Newton-Raphson must iterate, with each step improving the approximation as we get closer to the minimum where the quadratic assumption becomes more accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebaa7c-0ef9-4960-bd0a-5c59e9e1a9c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part E: Comparing Methods Across Different Starting Positions\n",
    "\n",
    "Now we'll test Newton-Raphson with the same three starting positions used in Part C and compare performance with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8330e8-7edd-42b7-a6ab-200b67e33a78",
   "metadata": {},
   "source": [
    "### Question 15: Test Newton-Raphson from multiple starting positions\n",
    "\n",
    "Test Newton-Raphson with starting positions r = 3.2 Å, r = 4.4 Å, and r = 6.0 Å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad62402-f77f-4160-8a30-4d9d1b2434ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Newton-Raphson from the same three starting positions\n",
    "starting_positions = [3.2, 4.4, 6.0]  # Å\n",
    "max_iterations = 50\n",
    "tolerance = 0.0005\n",
    "\n",
    "# Store results for Newton-Raphson\n",
    "results_nr = []\n",
    "\n",
    "for r_start in starting_positions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Newton-Raphson starting from r = {r_start} Å\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Initialize\n",
    "    positions_trial = [r_start]\n",
    "    gradients_trial = [lennard_jones_gradient(r_start, A, B)]\n",
    "    r_current = r_start\n",
    "    \n",
    "    # Newton-Raphson loop\n",
    "    converged = False\n",
    "    for iteration in range(max_iterations):\n",
    "        grad = lennard_jones_gradient(r_current, A, B)\n",
    "        second_deriv = lennard_jones_second_derivative(r_current, A, B)\n",
    "        \n",
    "        r_new = r_current - grad / second_deriv\n",
    "        \n",
    "        positions_trial.append(r_new)\n",
    "        gradients_trial.append(lennard_jones_gradient(r_new, A, B))\n",
    "        \n",
    "        if abs(gradients_trial[-1]) < tolerance:\n",
    "            converged = True\n",
    "            print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        r_current = r_new\n",
    "    \n",
    "    if not converged:\n",
    "        print(f\"Did not converge within {max_iterations} iterations\")\n",
    "    \n",
    "    # Store results\n",
    "    results_nr.append({\n",
    "        'r_start': r_start,\n",
    "        'r_final': positions_trial[-1],\n",
    "        'iterations': len(positions_trial) - 1,\n",
    "        'converged': converged,\n",
    "        'positions': positions_trial,\n",
    "        'gradients': gradients_trial\n",
    "    })\n",
    "    \n",
    "    print(f\"Final position: r = {positions_trial[-1]:.4f} Å\")\n",
    "    print(f\"Final gradient: |U'| = {abs(gradients_trial[-1]):.6f} eV/Å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af74299-2741-48b5-80c1-deb82b5d1441",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "**1. Newton-Raphson converges rapidly from r = 3.2 Å and r = 4.4 Å**\n",
    "\n",
    "For the first two starting positions, Newton-Raphson shows dramatic speed advantages:\n",
    "\n",
    "- **From r = 3.2 Å**: Newton-Raphson requires 6 iterations vs 96 for gradient descent (~16× faster)\n",
    "- **From r = 4.4 Å**: Newton-Raphson requires 4 iterations vs 16 for gradient descent (~4× faster)\n",
    "\n",
    "Newton-Raphson achieves this speed by using second-derivative information to predict the optimal step size at each iteration, rather than relying on a fixed learning rate. This allows it to take larger, more accurate steps directly towards the minimum. The speedup is particularly dramatic when starting from the repulsive wall (r = 3.2 Å), where gradient descent struggles with large gradients and requires many small steps to avoid overshooting.\n",
    "\n",
    "**2. Newton-Raphson fails completely from r = 6.0 Å**\n",
    "\n",
    "From r = 6.0 Å, Newton-Raphson converges to r = 6.96 Å after just 1 iteration. This is **not the minimum** - the algorithm has moved further away from the minimum at r ≈ 4.1 Å rather than towards it. Gradient descent, by contrast, successfully finds the minimum in 123 iterations.\n",
    "\n",
    "This failure occurs because **Newton-Raphson's fundamental assumption breaks down**. The method assumes the function curves upward (positive curvature, U'' > 0) like a bowl. However, at r = 6.0 Å we're in the long-range tail where the curvature is **negative** (U'' < 0) - the potential curves downward.\n",
    "\n",
    "When U'' < 0, the Newton-Raphson formula r_new = r_old - U'/U'' produces a step in the **wrong direction**. The negative curvature causes the algorithm to step uphill (towards larger r) rather than downhill towards the minimum. The algorithm then stops because the gradient at r = 6.96 Å happens to be below the convergence tolerance, even though this point is nowhere near the actual minimum.\n",
    "\n",
    "Gradient descent always moves downhill (in the direction of -U') regardless of curvature, making it slower but more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38518444-aa28-4468-b4f7-6d6996d7c354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
